{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Microfossils, Remote Sensing, and GIS for Proxy-dating Coastal Archaeological Sites and Landscapes: A case from Minas Basin, Bay of Fundy, Canada\n\nThis Jupyter Notebook is intended to replicate the production of geochronological *terminus ante quem* boundaries for the Kingsport marsh as described in the associated article. \n\nLoading this *AppendixB* Jupyter Notebook as a Binder increases reproducability of the workflow employed to create the geochronological boundaries.\n\nThe notebook can be broken down as follows:\n\n1. Import Packages\n\n\n2. CSV Data Import and Jupyter Functionality\n\n\n3. Correcting for Marine Resevoir Effect & Radiocarbon Calibration\n\n\n4. Correlating Sea Level Proxy Samples to Oak Point High Water\n\n\n5. Calculating Sea Level Rise (Higher High Water)\n\n\n6. Calculating Equvalent Past HHW at Oak Point\n\n\n7. Visualize Study Area\n\n\n8. Proxy Dating Rooted Stumps in a Drowned Forest\n\n## 1. Import Packages\n\nThe first step is to import the required Python packages to replicate the method presented in the associated article. \n\nRunning the following codeblock imports the required packages. "},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport iosacal\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport re\nimport os\nimport random\nimport requests\nimport datetime\n\nfrom ipyleaflet import *\n%matplotlib inline\n\nfrom pprint import pprint\nfrom numpy import ones,vstack\nfrom numpy.linalg import lstsq\nfrom iosacal import R, iplot\nfrom iosacal.text import single_text\nfrom ipywidgets import HTML\nfrom pandas import *\nfrom matplotlib import pyplot, figure\nfrom IPython.display import clear_output\nfrom random import choice\nfrom ipywidgets import interact, interact_manual, fixed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. CSV Data Import and Jupyter Functionality"},{"metadata":{},"cell_type":"markdown","source":"Following the data preparation steps outlined in the methods section of the associated article, we have placed a CSV titled **Appendix A**  within the */data* folder. The following codeblock will display the *AppendixA.csv* file as a table.\n<br><br>\nThe first line calls the CSV from the location within the project folder and loads it into a 'pandas' dataframe called *AppendixA*. Our second line defines formatting, cell alignment, and styles to be applied when we call the variable *Table*. *Table* is called in the third line and our formatted CSV is shown below."},{"metadata":{"trusted":false},"cell_type":"code","source":"#read CSV file to pandas dataframe\nAppendixA = pd.read_csv(\"./data/AppendixA.csv\")\n\n#create table style\nTable = AppendixA.style.format({\"error\": lambda x: \"±{:.1f}\".format(abs(x))}, na_rep=\"-\").format({\"date\": lambda x: \"{:.1f} C14BP\".format(float(x))}).format({\"core_depth\": lambda x: \"{:.2f}m\".format(abs(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_corrected\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_error\": lambda x: \"±{:.3f}m\".format(float(x))}, na_rep=\"-\").set_properties(subset=[\"error\", \"core_depth\", \"modern_HHW\", \"HHW_corrected\", \"HHW_error\"], **{'text-align': 'center'}).set_properties(subset=[\"date\"], **{'text-align': 'right'})\n\n#show sylized table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CSV data in the data frame can be quickly plotted by calling the attributes we wish to plot. The codeblock below plots the relationship of core depth to date of sample recovered from that depth in radiocarbon years."},{"metadata":{"trusted":false},"cell_type":"code","source":"#load date and sample depth values from CSV as lists\ndate = AppendixA['date']\ndepth = AppendixA['core_depth']\n\n#plot the list values as an x, y relationship\nplt.plot(date, depth);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once loaded into a data frame, records can be easily manipulated and displayed. This functionality is illustrated in the pertinent example that follows. "},{"metadata":{},"cell_type":"markdown","source":"#### Adjustment of error carried by Higher High Water values"},{"metadata":{},"cell_type":"markdown","source":"The values that loaded to the `AppendixA` data frame include a vertical uncertainty in the column `HHW_error`. This uncertainty reflects the vertical extent of the marsh zone indicated by Foraminifera within each radiocarbon dated sample (Smith et al. 1984). It is pre-loaded to the field carrying errors associated with the analysis to illustrate the convenience of combining documentation and calculations in Jupyter Notebooks. \n\nNot reported in the table above are uncertainties associated with values in the `modern_HHW` column. The reported bulk accuracy of the CANEAST dataset from where these points were derived is 7.5cm (Robin et al. 2016). The same publication notes that inconsistencies appear greater in the Bay of Fundy (Robin et al. 2016, Fig.5), though the difference between tide station and modelled lower low water elevation shown in our southwestern Minas Basin study area is within the range of reported uncertainty. Therefore we can be reasonably confident that the values and uncertainty provided by the CANEAST dataset reflect true uncertainty of higher high water at the time of publication, 2016.\n\nThe following cell adds the vertical uncertainty of the `modern_HHW` points to the uncertainty of the foram derived sample elevations we will be using for this analysis. The following code performs the operation by overwriting the values in the initial data frame with the result, therefore eliminating excess variable creation. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#replace values in HHW error column with the sum of itself and 0.0075 metres\nAppendixA[\"HHW_error\"] = AppendixA[\"HHW_error\"] + 0.075\n\n#add AOCENOVNVIKDJFLOD\nAppendixA[\"HHW_corrected\"] = AppendixA[\"modern_HHW\"]-AppendixA[\"core_depth\"]\n\n#show stylized table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the functionality of Jupyter Notebooks shown in the first step of this analysis. The next step is to prepare the radiocarbon dates for calibration."},{"metadata":{},"cell_type":"markdown","source":"## 3. Correcting for Marine Resevoir Effect & Radiocarbon Calibration\n\nNow that we have our CSV holding the necessary data to begin, we need to correct for marine resevoir effect, then calibrate our radiocarbon dates. Mentioned in the associated article, the ΔR of -64 ±90 (obtained from [CALIB](url=http://calib.org/marine/)) is subtracted from each radiocarbon date before calibration occurs. The codeblock below will perform this calculation. As per usual, when addition or subtraction between two values with reported errors, the errors are summed."},{"metadata":{"trusted":false},"cell_type":"code","source":"#apply marine resevoir correction to date and error values in table\nAppendixA[\"date\"] = AppendixA[\"date\"] - (-64)\nAppendixA[\"error\"] = AppendixA[\"error\"] + (90)\n\n#show stylized table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calibrating sample data"},{"metadata":{},"cell_type":"markdown","source":"The python package IOSACal is used in the following to calibrate the radiocarbon dates obtained from samples at Kingsport. \n\nThe values in the associated article use the MARINE20 calibration curve released in 2020. The most recent IOSACal version (v0.4.1) does not come with the MARINE20 or INTCAL20 .14c calibration curve files, but were manually added to a local distribution for publication purposes. In the following, the exact same process is run on the same samples using the MARINE13 calibration curve file. Slightly different results ensue, but the code will be updated to reference the new calibration curve once IOSACal v0.5.0 is released.\n\nRecords from the CSV file are used as input for a loop calibrating the input dates in the following radiocarbon calibration. The output is a short report and a probability distribution plot for each of the calibrations.\n\nTo prepare for this process we create variables to hold the inputs and outputs of the calibration in the following codeblock. first three lines of code define the variables `s`, `d`, and `e` as values from specified columns in our dataframe (`ID`, `date`, & `error`). The following two lines initialize empty lists, `cal68` and `cal95`, that will store the minimum and maximum date of each confidence interval for each sample."},{"metadata":{"trusted":false},"cell_type":"code","source":"#create variables for sample id, date, and date error\ns = AppendixA.ID.values\nd = AppendixA.date.values\ne = AppendixA.error.values\n\n#create a list to hold min and max values of probability distributions\ncal68 = []\ncal95 = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot worthy of explaination happens in the codeblock.\n\nBeginning with line 1, the following text described the process of our loop line by line.\n\nLine 1 is the header of our `for` loop. The header defines the parameters of our loop, and describes the conditions to be met `for` the loop to stop. Our conditions state iterate the body of the loop below `for` a number of times equal to the `range` of `axis` `0`, or `rows`, in `AppendixA`. We have three samples, so the loop will iterate a total of three times with the provided data.<br>\n\nLine 2 begins the body of the loop by defining a variable called `r` as equal to the values of date (`d`), error (`e`), and ID (`s`). The variables are designated as radiocarbon dates with the variable `R` (shorthand in IOSACal). The `i` in square brackets following `d`, `e`, and `s` is a reference to the value of `date`, `error`, and `ID` for the record with an index matching the current iteration of the loop. The `r` variable now holds a list containing the three values for each of the three samples. <br>\n\nLine 3 passes the values of our list stored in `r` to the `calibrate` method in IOSACal. By prefixing `r` to `.calibrate` we are defining that the values in `r` will will be calibrated using the selected calibration curve. The calculated values are saved in the variable `cal_r`. <br>\n\nLine 4 creates a report for the calibration, saved to the variable - `report`.<br>\n\nOn line 5, we search the output report text for digits, storing them in as a list in the variable called `findDigits`. Line 6 and 7 append the min and max values from our `findDigits` list for the 68% and 95% confidence intervals, based on their position in the list. These lines take advantage of the consistent structure IOSACal uses for reporting to allow us to directly use the outputs in further analysis.<br>\n\nLines 8 and 9 then `print` the report stored in our `report` variable below our code, and creates and saves a visualization of the calibration to the directory - `/SupplementaryData/output` as a JPG image named by `ID`.\n\n**REMINDER:** *the most recent release of IOSACal (v.0.4.1) does not include the MARINE20 calibration curve. This notebook currently uses MARINE13 for radiocarbon calibration, while dates reported within the article use MARINE20. This results in slight variances between MARINE20 dates in the article and MARINE13 dates produced below. Upon release, IOSACal v0.5.0 will be automatically installed with the Binder image running this notebook, and code will promptly be updated to make use of the most recent dataset.*"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#define the output directory\ndir= r\"./output/\"\n\n#begin for loop iterating through all input records \nfor i in range(AppendixA.shape[0]):\n    \n    #sort date, error, and sample id records and add input radiocarbon sample list\n    r = R(d[i], e[i], s[i])\n    \n    #calibrate dates with the calibration curve shown in quotes\n    cal_r = r.calibrate('marine13')\n    \n    #create text based output report for radiocarbon calibration\n    report = (single_text(cal_r))\n    \n    #add all integer only words of one of more digits in the report to a list\n    findDigits = [int(s) for s in re.findall(r'\\b\\d+\\b', report)]\n    \n    #append the resulting list values to new lists\n    #68% confidence interval values are location 7 and 8\n    #95% confidence interval values are location 13 and 14\n    cal68.append(findDigits[7:9])\n    cal95.append(findDigits[13:15])\n    \n    #print report for current sample as output\n    print(report)\n    \n    #show probability distribution plot as inline plot below output\n    iplot(cal_r, output = dir + s[i] + \".jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After performing radiocarbon calibration, the output lists are cast to a `numpy array` for storage as float values. These values are put into a `for` loop that iterates for `i` number of times. The number of iterations is set using the `range()` function with an input parameter of a list of values. For flexibility on the number of records that can be input to this code this value is equal the the number of records in our input table by calling the shape of any column with `AppendixA.shape[0]` (here we chose the first column `0`).\n\nWithin the `for` loop, simple calculations run over 1σ (68%) and 2σ (95%) confidence intervals to retrieve maximum, minimum, mean and associated uncertainty of each sample in calibrated years before present (CalBP) . These values are appended to the variables `cal95Date`,  `cal95Mean`,  `cal95Err`, `d`, and `e` for use later in the workflow."},{"metadata":{"trusted":false},"cell_type":"code","source":"#create array objects to hold values not created above\ncal68Date = np.array(cal68)\ncal95Date = np.array(cal95)\ncal95Mean = np.array(d)\ncal95Err = np.array(e)\n\n#begin for loop iterating through all input records \nfor i in range(AppendixA.shape[0]):\n    \n    #calculate mean and uncertainty for 68% confidence intervals\n    d[i] = (cal68Date[i,0]+cal68Date[i,1])/2\n    e[i] = (cal68Date[i,0]-cal68Date[i,1])/2\n    \n    #calculate mean and uncertainty for 95% confidence intervals\n    cal95Mean[i] = (cal95Date[i,0]+cal95Date[i,1])/2\n    cal95Err[i] = (cal95Date[i,0]-cal95Date[i,1])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results of the radiocarbon calibration are viewed in more detail below. The following codeblock creates and displays a new data frame formatted to show date range, mean, and uncertainty at both confidence intervals for all samples calibrated. The new data frame `calDates` "},{"metadata":{"trusted":false},"cell_type":"code","source":"#define columns of radiocarbon calibration table\ncols = ['2'+'\\u03C3'+' Max Age', '2'+'\\u03C3'+' Min Age']\ncols2 = ['1'+'\\u03C3'+' Max Age', '1'+'\\u03C3'+' Min Age']\ncols3 = ['2'+'\\u03C3'+' Mean Age', '2'+'\\u03C3'+' Uncertainty', '1'+'\\u03C3'+' Mean Age', '1'+'\\u03C3'+' Uncertainty'] \n\n#create an array holding mean date and uncertainty at each confidence interval\ncalMean = np.column_stack((cal95Mean,cal95Err,d,e))\n\n#create variable to hold an int of the number of records\nnumRecs = len(AppendixA)\n\n#load cal95 list to a data frame and add column titles\ncalDates = pd.DataFrame(cal95)\ncalDates.columns = cols\n\n#add remaining datasets and column titles\ncalDates.reindex(columns=calDates.columns.tolist() + cols2)\ncalDates[cols2] = cal68\ncalDates.reindex(columns=calDates.columns.tolist() + cols3)\ncalDates[cols3] = calMean.reshape(numRecs,4)\ncalDates.insert(0, \"Sample ID\", AppendixA.iloc[0:numRecs,0])\n\n#stylize and show full results of radiocarbon calibration in a table\ncalDates.style.hide_index().set_properties([], **{'text-align': 'center'}).format({'2'+'\\u03C3'+' Max Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'2'+'\\u03C3'+' Min Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'1'+'\\u03C3'+' Mean Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'2'+'\\u03C3'+' Mean Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'1'+'\\u03C3'+' Max Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'1'+'\\u03C3'+' Min Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'2'+'\\u03C3'+' Uncertainty': lambda x: \"±{:.0f} \".format(float(x))}).format({'1'+'\\u03C3'+' Uncertainty': lambda x: \"±{:.0f} \".format(float(x))}).set_properties(subset=['2'+'\\u03C3'+' Uncertainty','1'+'\\u03C3'+' Uncertainty'], **{'text-align': 'left'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *Section out of place for now*. Calculating Sea Level Rise (Higher High Water)"},{"metadata":{},"cell_type":"markdown","source":"Our calculated HHW values for the Kingsport sample dates are shown above.\n\nNext step is to determine the amount of RSL rise that has occurred since 1980 at the Kingsport sample locations. In order to do this, two coordinate pairs are required. Variables are defined below for our `x1`, `x2`, `y1`, and `y2` coordinates. Variable `x1` represents the radiocarbon equivalent date for the year 2016, where the value 0 is equal to the year 1950."},{"metadata":{"trusted":false},"cell_type":"code","source":"#define the year today and year of hhw publication\nyrToday = datetime.datetime.now().year\nyrPub = 2016\n\n#define modern date and errors to append to new data frame\nconfToday = [[1950-yrPub,0,1950-yrPub,0]]\n\n#create list of hhw from elevations\nhhw = pd.read_csv(\"./data/hhw.csv\")\nhhw = list(hhw.iloc[0:4,3])\n\n#prepare data in data frame for processing\ncoordData = pd.DataFrame(calMean)\ncoordData = coordData.join(AppendixA)\ncoordData = coordData.drop(['ID', 'date', 'error','core_depth', 'modern_HHW'], axis=1)\ncoordData = coordData.append(confToday)\ncoordData = coordData.fillna(0)\ncoordData.iloc[3:4,4] = coordData.iloc[3:4,4] + float(hhw[3])\ncoordData.iloc[3:4,5] = coordData.iloc[3:4,5] + 0.2\ncoordData = coordData.reset_index(drop=True)\ncoordData","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use our coordinate pairs from our table to derive the equation for the linear relationship between the most recent of the radiocarbon dated samples (*Gx-6812, HHW_corrected*) and modern HHW for that sample (*Gx-6812, modern_HHW*) following a  slightly modified codeblock [\"from Stack Overflow\"](https://stackoverflow.com/a/21566184). This equation will be used to solve for the HHW in the year 1980 by solving for x = -30 in the two following codeblocks. \n\nThe first codeblock creates an object called *coords* from our *x1, y1*, and *x2, y2* coordinate pairs of date and elevation. We then use <b>zip()</b> to sort these pairs into *x_coords* and *y_coords* lists. The third line creates the coefficient matrix (*A*) for the least squares method, <b>lstsq()</b>, on line 4. The two derived values are the slope (<i>m</i>) and intercept (<i>c</i>) of our equation, printed below."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"mList, bList, oldList, yngList = [],[],[],[]\niters = len(coordData.index)\ni = 1\ninterval = 68\n\nwhile i < iters:\n    iters = len(coordData.index)\n    #define x1 and x2 as sequential high water values\n    if interval == 95:\n        x1 = coordData.loc[iters-i].at[0]\n        x2 = coordData.iloc[iters-i-1,0]\n    else:\n        x1 = coordData.loc[iters-i].at[2]\n        x2 = coordData.loc[iters-i-1].at[2]\n    y1 = coordData.loc[iters-i].at[\"HHW_corrected\"]\n    y2 = coordData.loc[iters-i-1].at[\"HHW_corrected\"]\n    \n    #define a list of coordinate pairs\n    coords = [(x1,y1),(x2,y2)]\n\n    #re-arrange the lists then add to an array\n    x_coords, y_coords = zip(*coords)\n    A = vstack([x_coords,ones(len(x_coords))]).T\n\n    #define variables m and b as the result of least-squares \n    #regression using the input coordinate pairs\n    m, b = lstsq(A, y_coords, rcond=None)[0]\n    yngList.append(x1)\n    oldList.append(x2)\n    mList.append(m)\n    bList.append(b)\n\n    #print the equation\n    print(\"Equation for {:.0f} to {:.0f} CalBP is y = {:.5f}x + {:.5f}\".format(x1,x2,m,b))\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our output equation is: $$y = -0.00230x + 6.63404$$<br>\n\nThe following two codeblocks perform two calculations. First, a modeled HHW elevation value is calaculated for Kingsport in 1980 by solving for <i>y</i>, where: $$x = -30$$<br>  Next, RSL rise since 1980 at HHW is calculated from the difference in HHW compared to 2020.<br>\n\nThe following codeblock can also be expressed as: $$HHW_{1980} = 0.00230(-30) + 6.63404$$<br>\n\nOur sea level rise calculation in the second codeblock is expressed as: $$RSLR_{[1980,2020]} = HHW_{2020} - (0.00230(-30) + 6.6340)$$<br>\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"yr=2021\ninYr = (1950-yr)\n\nSL = mList[0] * (inYr) + bList[0]\n\nprint(\"Higher high water was\", \"{:.3f}m\".format(SL),\"CGVD2013 at Oak Point in {:.0f}.\".format(yrToday))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"hhwSmp = pd.read_csv(\"./data/hhw.csv\", usecols=[\"ID\"])\nhhwZ = pd.read_csv(\"./data/hhw.csv\", usecols=[\"hhw\"])\nhhw = hhwSmp.join(hhwZ)\n\n#cast list to dictionary for input to widgets below\nhhw = hhw.to_dict('split')\n\ndd = widgets.Dropdown(\n    options=hhw['data'],\n    description='Modern HHW sample locations:',\n    style={'description_width': 'initial'},\n    value= hhw['data'][0][1]\n)\n\ntext = widgets.Text(\n    placeholder='select high water location',\n    description='Reported elevation:',\n    style={'description_width': 'initial'},\n    value= str(hhw['data'][0][1]) + \" m CGVD2013\",\n    disabled=False\n)\n\nsldMax = float()\n\ndef value_changed(change):\n    sldMax = change.new\n    text.value = str(change.new) + \" m CGVD2013\"\n    \ndd.observe(value_changed, 'value')\n\nwidgets.HBox([dd, text])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#THIS CELL GIVES IMPRECISE VALUES FOR NOW\n\nSLR = AppendixA.loc[2].at[\"modern_HHW\"] - SL\nSLRcm = SLR*100\n\nprint(\"Sea level has risen\", \"{:.1f}cm\".format(SLRcm),\"since 1980 at Oak Point.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As our <i>HHW_corrected</i> elevations for the Kingsport core samples are calculated as values for the year 2020, we need to correct the elevations to match depth below 1980 HHW. The calculation is performed in the codeblock below and expressed as: $$HHW\\_corrected_{AppendixA} = HHW\\_corrected_{AppendixA} - SLR$$"},{"metadata":{"trusted":false},"cell_type":"code","source":"AppendixA[\"HHW_corrected\"] = AppendixA[\"HHW_corrected\"] - SLR\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Correlating Sea Level Proxy Samples to Oak Point High Water\n\n#### Calculating Sample Higher High Water\n\n##### REDO TEXT IN THIS SECTION. SHOULD ACTUALLY *ONLY* CREATE OAK POINT DATA FRAME WITH HHW VALUE FOR OAK POINT OFFSET BY DIFFERENCE BETWEEN CORE DEPTH AND HHW POINTS CLOSEST TO EACH SAMPLE. THIS OFFSET VALUE IS EQUAL TO THE DEPTH BELOW HIGHER HIGH WATER AT THE LOCATION THE CORE WAS TAKEN. THIS IS POSSIBLE BECAUSE BOTH HIGHER HIGH WATER VALUES ARE GRIDDED SAMPLE POINTS DERIVED FROM THE CANEAST DIGITAL OCEAN MODEL DATASET.\n\nWith radiocarbon calibration complete, production of the geochronological *terminus ante quem* boundaries can begin. Calculations shown below rely only on the 1σ (68%) confidence interval, while both intervals are used later to visualize the relative sea level over time.\n\nFirst step is to calculate the elevation of the radiocarbon dated samples. These elevations will be used to derive past HHW in the CGVD2013 vertical datum.\n\nThe codeblock below subtracts *core_depth* from *modern_HHW* and places the result in the *HHW_corrected* column."},{"metadata":{"trusted":false},"cell_type":"code","source":"#calculate past high water elevation as current high water subtract depth of sample\nAppendixA[\"HHW_corrected\"] = AppendixA[\"modern_HHW\"]-AppendixA[\"core_depth\"]\n\n#restyle the Table dataframe to output CalBP\nTable = AppendixA.style.hide_index().format({\"error\": lambda x: \"±{:.1f}\".format(abs(x))}, na_rep=\"-\").format({\"date\": lambda x: \"{:.0f} CalBP\".format(float(x))}).format({\"core_depth\": lambda x: \"{:.2f}m\".format(abs(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_corrected\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_error\": lambda x: \"±{:.3f}m\".format(float(x))}, na_rep=\"-\").set_properties(subset=[\"error\", \"core_depth\", \"modern_HHW\", \"HHW_corrected\", \"HHW_error\"], **{'text-align': 'center'}).set_properties(subset=[\"date\"], **{'text-align': 'left'})\n\n#show formatted table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sample `Gx-6811` was identified as representing marsh Zone 1b, in the lower portion of high marsh. These marsh zones were identified through surface sampling vegetation and Foraminifera along transects within the marsh (Smith et al, 1984). Marsh zone 1b was defined at Kingsport as having a vertical range of 55cm, and a maximum of 75cm below high tide. This means that at a minimum, the upper limit of Zone 1b will be 20cm below the limit of high water.\n\nTo adjust for the vertical position of sample `Gx-6811` in marsh Zone 1b, an offset of 20cm is added. This increases the elevation in the `HHW_corrected` field to represent the high water mark defining the limit of marsh Zone 1a. The codeblock below performs this correction, then displays the updated values in our `AppendixA` dataframe."},{"metadata":{"trusted":false},"cell_type":"code","source":"#apply positive offset to account for the upper\n#limit of zone 1b being 20cm below high-water\nAppendixA.iloc[1:2,5] = AppendixA.iloc[1:2,5]+0.2\n\n#show formatted table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Equvalent Past HHW at Oak Point\n\nAn equivalent HHW elevation at Oak Point is obtained for each calibrated date by correcting for SLR and sample elevation.\n\nAn offset value for the geochronological boundary elevations at Oak Point is provided by the difference in RSL rise for the Kingsport sample because we can expect our high-marsh indicators to increase in elevation at the rate of RSL rise. \n\nThe following codeblock calculates equivalent modern HHW elevations for Oak Point.\n\nOn line 1 the HHW value of `6.826` is used to calculate elevations for each sample subtracting <i>core_depth</i> and <i>SLR</i>. \n\nLine 2 displays the list of calculated values within <i>OakPointHHW</i>."},{"metadata":{"trusted":false},"cell_type":"code","source":"OakPointHHW = [(6.826 - AppendixA[\"core_depth\"] - SLR)-(6.826-AppendixA[\"modern_HHW\"])]\nOakPointHHW","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following codeblock creates a new data frame called `OakPoint`. This data frame relates the newly calculated HHW elevations for Oak Point with the calibrated dates, and errors from samples adjacent to the archaeological site.\n\nTo do this the new `OakPoint` data frame is created from a copy of our `AppendixA` data frame. The `AppendixA` HHW values are replaced with values calculated from the HHW taken from `OakPoint` data frame with values from the `OakPointHHW` list calculated above. \n\nLine 3 drops the unnecessary fields from the `AppendixA` data frame.\n\nLines 4 and 5 stylize out table, and display the result."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#create OakPoint data frame from AppendixA data frame\nOakPoint = pd.DataFrame(data=AppendixA)\n\n#replace the HHW_corrected values with values from OakPointHHW - calculated above\nOakPoint.iloc[0:3,5] = OakPointHHW[0]\n\n#drop the ID, core_depth, and modern_HHW columns\nOakPoint = OakPoint.drop(['ID','core_depth','modern_HHW'], axis=1)\n\n#positive offset for foram. zone 1b adjusting to HHW value\nOakPoint.iloc[1:2,2] = OakPoint.iloc[1:2,2]+0.2\n\n#format the table\nOakPointTable = OakPoint.style.hide_index().format({\"error\": lambda x: \"±{:.1f}\".format(abs(x))}, na_rep=\"-\").format({\"date\": lambda x: \"{:.1f} CalBP\".format(float(x))}).format({\"HHW_corrected\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_error\": lambda x: \"±{:.2f}m\".format(float(x))}, na_rep=\"-\").set_properties(subset=[\"error\", \"HHW_corrected\", \"HHW_error\"], **{'text-align': 'center'}).set_properties(subset=[\"date\"], **{'text-align': 'left'})\n\n#show formatted table\nOakPointTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Visualize Study Area\n\nAn interactive map frame powered by iPyLeaflet is prepared by running the proceeding code block. The map frame is centered on the Oak Point study area at Kingsport marsh.\n\nWe define the map frame as *mapFrame* on line 1 followed by adding data and functionality: \n\n- ESRI satellite imagery basemap\n- custom map tile layer from the lidar basemaps shown in the article\n- measurement functionality\n- full screen functionality\n- scale bar\n- layer toggle functionality"},{"metadata":{"trusted":false},"cell_type":"code","source":"#initialize map as \"mapFrame\"\nmapFrame = Map(basemap=basemaps.Esri.WorldImagery, center=(45.1562107, -64.3699627), zoom=15, max_zoom=18)\n\n#load and add local tile layer\nbasemap = LocalTileLayer(name=\"Shaded Relief\",tms = True, attribution=\"Contains information licensed under the Open Government Licence – Nova Scotia. Created by: Wesley Weatherbee.\", path=\"minasBase/{z}/{x}/{y}.png\")\nmapFrame.add_layer(basemap)\n\n#load and add measurement function, fullscreen and layer toggle, and a visual scale\nmeasure = MeasureControl(position='topleft',active_color = 'orange',primary_length_unit = 'meters')\nmapFrame.add_control(measure)\nmeasure.completed_color = 'red'\nmapFrame.add_control(FullScreenControl(position='topright'))\nmapFrame.add_control(LayersControl(position='topleft'))\nmapFrame.add_control(ScaleControl(position='topright', metric=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creation of the map data"},{"metadata":{},"cell_type":"markdown","source":"The open map above displays datasets that will help explore the output boundaries after running this notebook. The extent of the shaded relief mirrors that of Figure 1 in the associated article presenting a high resolution maximum zoom level of 18 throughout.\n\nThe shaded relief layer was created from Lidar derived digital elevation models at 1m resolution from the [Province of Nova Scotia](url: \"https://nsgi.novascotia.ca/datalocator/elevation/\"). The layer is a series of compressed .png tiles within an organized folder directory using naming conventions relating to geolocation. This directory is located at */minasBase* off of the Binder root folder. \n\nThe */minasBase* directory was automatically created from a Map Tile Package (.tpk) file exported from ArcGIS Pro. With the help of python modules `tpkutils` and `mbutil`, the .tpk was converted to a .mbtiles, MapBox tile package file then extracted to the */minasBase* directory. \n\nOptimization of the map tiles is responsible for the high zooom level display presented in this Binder. An original file count for the */minasBase* map tiles directory exceeded 5 million, which was not appreciated by GitHub or Binder. However, with the help of native python modules, the file count was reduced to approximately 37000 by removing any .png images in the directory that were a size of 190 bytes or smaller—files of this size contained no image data, only a transparency layer. \n\n#### Loading surveyed elevations of sea level proxy observations\n\nSurveyed points have been gathered at Oak Point and throughout the mudflats of Minas Basin. Below we will load and visualize some survey data to the map frame. \n\nTo differentiate the values a `Popup` method is used to visualize the `properties.Code` property of the points when a point is clicked. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"#load geojson file containing surveyed but undated rsl indicator points\nwith open(r\"./data/rslPoints.geojson\", 'r') as f:\n    data = json.load(f)\n\n#reference geojson feature ID as variable 'features'\nfeatures = data['features']\nindElev = []\nzErr = []\n\n#create layer group referencing json records within the map frame\nrslGroup = LayerGroup(name=\"RSL Indicators\", layers=())\n\n#loop over geojson features\nfor i in range(len(features)):\n    location=(features[i]['geometry']['coordinates'][1],features[i]['geometry']['coordinates'][0])\n    indicators = features[i]['properties']['Code']\n    indElev.append(features[i]['properties']['OHeight'])\n    zErr.append(features[i]['properties']['zError'])\n    ind = str(indicators)\n    html = \"\"\"<p><b>Indicator type: </b>\"\"\" + f\"{ind}\" + \"\"\"</p>\"\"\"\n    rslPoint = CircleMarker( radius=4, color=\"#000000\", location=location, weight=1, opacity=1, \n                             fill_opacity=0.8, fill_color=\"#80bfff\", draggable=False)\n    \n    # Popup associated to a layer\n    rslPoint.popup = HTML(html)\n    rslGroup.add_layer(rslPoint)\n\n#add group layer created from combined rslPoint layers\nmapFrame.add_layer(rslGroup)\n\n#call and display the map frame\nmapFrame   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Proxy Dating Rooted Stumps in Drowned Forest\n\nThe following were used to produce a plot of surveyed locations of stumps in the drowned forest in relation to sea level rise over-time. The cell below migrates calculated values from the *OakPoint* data frame in to a list. Current organization of the workflow required modern high water elevation and errors to be manually entered. Though this can be easily changed in the future."},{"metadata":{"trusted":false},"cell_type":"code","source":"#load x values to plot from the OakPoint data frame  \n#then append the modern date relative to 1950\nx = list(coordData[2])\n\n#load date uncertainty values to plot from the OakPoint \n#data frame  then append the modern date uncertainty\ndateErr = list(coordData[3])\nerr98 = list(coordData[1])\n\n#load y values to plot from the OakPoint data frame  \n#then append the modern high water and uncertainty\ny = list(coordData['HHW_corrected'])\n\nerror = list(coordData['HHW_error'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some placehoplder text for now."},{"metadata":{"trusted":false},"cell_type":"code","source":"indAge = []\nfor i in range(len(features)):\n    if indElev[i] >= y[2]:\n        indAge.append((indElev[i]-bList[0])/mList[0])\n    elif indElev[i] >= y[1]:\n        indAge.append((indElev[i]-bList[1])/mList[1])\n    elif indElev[i] >= y[0]:\n        indAge.append((indElev[i]-bList[2])/mList[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we create a plot using the same custom style used in the associated article. \n\nThe cell below converts the lists above in to `numpy array` objects, obtains the values from the geojson indicator points we stored in variables previously, and plots the values using custom style parameters."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#use the seaborn style colorblind in the plot\nplt.style.use('seaborn-colorblind')\n\n#define method to round values to the nearest 5\ndef myround(x, base=5):\n    return int(base * round(float(x)/base))\n\n#define x, y, errors, indicator elevation, z-error, calculated date, and date error\nx = np.array(x)\ndateErr = np.array(dateErr)\ny = np.array(y)\nerror = np.array(error)\nerr98 = np.array(err98)\nindAge = np.array(indAge)\n\n#plot points, errorbars, interpolated rsl indicator age, and errorbar bounds\nplt.plot(x, y, 'bo', x, y, 'k', linewidth=1.25, linestyle='dashed')\nplt.plot(indAge, indElev, 'd', color='#4CE600', markersize=6, markeredgewidth=0.75, markeredgecolor='k')\nplt.fill_betweenx(y, x-err98, x+err98, alpha=0.2, edgecolor='#A80000', facecolor='#A83800',\n    linewidth=1.5, linestyle='dashdot', antialiased=True)\nplt.fill_betweenx(y, x-dateErr, x+dateErr, alpha=0.4, edgecolor='#A80000', facecolor='#A83800',\n    linewidth=1.5, linestyle='dashdot', antialiased=True)\nplt.errorbar(x,y,error,dateErr, alpha=0.6, color='#1B2ACC', linewidth=0, elinewidth=1.5, ecolor='black', capsize=5)\nplt.fill_between(x, y-error, y+error, alpha=0.75, edgecolor='#1B2ACC', facecolor='#51A8C7',\n    linewidth=1.25, linestyle='dashdot', antialiased=True)\n\n#stylize ticks, gridlines, bounds, and axis labels\nplt.xticks(np.arange(0,5000, step=500))\nplt.yticks(np.arange(-4,7.5, step=0.5))\nplt.axhline(-4,color='black')\nplt.axvline(-250,color='black')\nplt.axhline(7.5,color='black')\nplt.axvline(5000,color='black')\nplt.tick_params(direction='in', length=6, width=1, colors='black',grid_color='gray', grid_alpha=0.5)\nplt.xlabel('Age (years CalBP)')\nplt.ylabel('Elevation (metres CGVD2013)')\nplt.xlim(-250, 5000)\nplt.ylim(-4, 7.5)\n\n# Show the major grid lines with dark grey lines\nplt.grid(b=True, which='major', color='#666666')\n\n# Show the minor grid lines with very faint and almost transparent grey lines\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='#999999', alpha=0.2)\n\n#plot the figure and set the size\nfig = plt.gcf()\nfig.set_size_inches(20, 12)\nplt.savefig(r'./output/seaLevelCurve.png', edgecolor='black', dpi=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result of our above plot shows the relative sea-level curve for Oak Point. The calibrated dates were produced with the calibration curve used in the above process, and elevations were corrected using HHW points from a CHS grid surface for higher high water. Uncertainties associated with each point are shown with vertical and horizontal bars.\n\nA linear model is used to show interpolated uncertainty bounds over time. The light red boundary shows uncertainty of the radiocarbon calibration in the 95% confidence interval, while the 68% confidence interval is more opaque. Shown in blue boundary is uncertainty associated with the foram assemblage derived relative sea level indicator points.\n\nThere it is. Using the convenience of Jupyter Notebooks, powerful functionality of Python, and accessibility of MyBinder the Supplementary Data has interactively reproduced the analysis required to obtain a *terminus ante quem* proxy date from surveyed observations of coastal archaeological sites and landscapes."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}