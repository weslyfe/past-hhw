{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Microfossils, Remote Sensing, and GIS for Proxy-dating Coastal Archaeological Sites and Landscapes: A case from Minas Basin, Bay of Fundy, Canada\n\nThis Jupyter Notebook is intended to replicate the production of geochronological *terminus ante quem* boundaries for the Kingsport marsh as described in the associated article. \n\nLoading this *AppendixB* Jupyter Notebook as a Binder increases reproducability of the workflow employed to create the geochronological boundaries.\n\nThe notebook can be broken down as follows:\n\n1. Import Packages\n\n\n2. Visualize Study Area\n\n\n3. CSV Data Import\n\n\n4. Marine Resevoir Correction & Radiocarbon Calibration\n\n\n5. Elevation Correction of Sea Level Proxy Samples\n\n\n6. Calculating Sea Level Rise (Higher High Water)\n\n\n7. Calculating Equvalent Past HHW at Oak Point\n\n\n8. Proxy Dating Rooted Stumps in a Drowned Forest\n\n## 1. Import Packages\n\nThe first step is to import the required Python packages to replicate the method presented in the associated article. \n\nRunning the following codeblock imports the required packages. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport iosacal\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport re\nimport os\nimport random\nimport requests\n\nfrom ipyleaflet import *\n%matplotlib inline\n\nfrom pprint import pprint\nfrom numpy import ones,vstack\nfrom numpy.linalg import lstsq\nfrom iosacal import R, iplot\nfrom iosacal.text import single_text\nfrom ipywidgets import HTML\nfrom pandas import *\nfrom matplotlib import pyplot, figure\nfrom IPython.display import clear_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Visualize Study Area\n\nAn interactive map frame powered by iPyLeaflet is prepared by running the proceeding code block. The map frame is centered on the Oak Point study area at Kingsport marsh.\n\nWe define the map frame as *mapFrame* on line 1 followed by adding data and functionality: \n\n- ESRI satellite imagery basemap\n- custom map tile layer from the lidar basemaps shown in the article\n- measurement functionality\n- full screen functionality\n- scale bar\n- layer toggle functionality"},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize map as \"mapFrame\"\nmapFrame = Map(basemap=basemaps.Esri.WorldImagery, center=(45.1562107, -64.3699627), zoom=15, max_zoom=18)\n\n#load and add local tile layer\nbasemap = LocalTileLayer(name=\"Shaded Relief\",tms = True, attribution=\"Contains information licensed under the Open Government Licence – Nova Scotia. Created by: Wesley Weatherbee.\", path=\"minasBase/{z}/{x}/{y}.png\")\nmapFrame.add_layer(basemap)\n\n#load and add measurement function, fullscreen and layer toggle, and a visual scale\nmeasure = MeasureControl(position='topleft',active_color = 'orange',primary_length_unit = 'meters')\nmapFrame.add_control(measure)\nmeasure.completed_color = 'red'\nmapFrame.add_control(FullScreenControl(position='topright'))\nmapFrame.add_control(LayersControl(position='topleft'))\nmapFrame.add_control(ScaleControl(position='topright', metric=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we show the map frame by calling the variable referencing that data by running the following:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#show \"mapFrame\" map\nmapFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creation of the map data"},{"metadata":{},"cell_type":"markdown","source":"The open map above displays datasets that will help explore the output boundaries after running this notebook. The extent of the shaded relief mirrors that of Figure 1 in the associated article presenting a high resolution maximum zoom level of 18 throughout.\n\nThe shaded relief layer was created from Lidar derived digital elevation models at 1m resolution from the [Province of Nova Scotia](url: \"https://nsgi.novascotia.ca/datalocator/elevation/\"). The layer is a series of compressed .png tiles within an organized folder directory using naming conventions relating to geolocation. This directory is located at */minasBase* off of the Binder root folder. \n\nThe */minasBase* directory was automatically created from a Map Tile Package (.tpk) file exported from ArcGIS Pro. With the help of python modules `tpkutils` and `mbutil`, the .tpk was converted to a .mbtiles, MapBox tile package file then extracted to the */minasBase* directory. \n\nOptimization of the map tiles is responsible for the high zooom level display presented in this Binder. An original file count for the */minasBase* map tiles directory exceeded 5 million, which was not appreciated by GitHub or Binder. However, with the help of native python modules, the file count was reduced to approximately 37000 by removing any .png images in the directory that were a size of 190 bytes or smaller—files of this size contained no image data, only a transparency layer. "},{"metadata":{},"cell_type":"markdown","source":"## 3. CSV Data Import"},{"metadata":{},"cell_type":"markdown","source":"Following the data preparation steps outlined in the methods section of the associated article, we have placed a CSV titled **Appendix A**  within the */data* folder. The following codeblock will display the *AppendixA.csv* file as a table.\n<br><br>\nThe first line calls the CSV from the location within the project folder and loads it into a 'pandas' dataframe called *AppendixA*. Our second line defines formatting, cell alignment, and styles to be applied when we call the variable *Table*. *Table* is called in the third line and our formatted CSV is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#read CSV file to pandas dataframe\nAppendixA = pd.read_csv(\"./data/AppendixA.csv\")\n\n#create table style\nTable = AppendixA.style.format({\"error\": lambda x: \"±{:.1f}\".format(abs(x))}, na_rep=\"-\").format({\"date\": lambda x: \"{:.1f} C14BP\".format(float(x))}).format({\"core_depth\": lambda x: \"{:.2f}m\".format(abs(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_corrected\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_error\": lambda x: \"±{:.2f}m\".format(float(x))}, na_rep=\"-\").set_properties(subset=[\"error\", \"core_depth\", \"modern_HHW\", \"HHW_corrected\", \"HHW_error\"], **{'text-align': 'center'}).set_properties(subset=[\"date\"], **{'text-align': 'right'})\n\n#show sylized table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CSV data in the data frame can be quickly plotted by calling the attributes we wish to plot. The codeblock below plots the relationship of core depth to date of sample recovered from that depth in radiocarbon years."},{"metadata":{"trusted":true},"cell_type":"code","source":"#load date and sample depth values from CSV as lists\ndate = AppendixA['date']\ndepth = AppendixA['core_depth']\n\n#plot the list values as an x, y relationship\nplt.plot(date, depth);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Marine Resevoir Correction & Radiocarbon Calibration"},{"metadata":{},"cell_type":"markdown","source":"Now that we have our CSV holding the necessary data to begin, we need to correct for marine resevoir effect, then calibrate our radiocarbon dates. Mentioned in the associated article, the ΔR of -64 ±90 (obtained from [CALIB](url=http://calib.org/marine/)) is subtracted from each radiocarbon date before calibration occurs. The codeblock below will perform this calculation. As per usual, when addition or subtraction between two values with reported errors, the errors are summed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply marine resevoir correction to date and error values in table\nAppendixA[\"date\"] = AppendixA[\"date\"] - (-64)\nAppendixA[\"error\"] = AppendixA[\"error\"] + (90)\n\n#show stylized table\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calibrating Sample Data"},{"metadata":{},"cell_type":"markdown","source":"The python package IOSACal is used in the following to calibrate the radiocarbon dates obtained from samples at Kingsport. \n\nThe values in the associated article use the MARINE20 calibration curve released in 2020. The most recent IOSACal version (v0.4.1) does not come with the MARINE20 or INTCAL20 .14c calibration curve files, but were manually added to a local distribution for publication purposes. In the following, the exact same process is run on the same samples using the MARINE13 calibration curve file. Slightly different results ensue, but the code will be updated to reference the new calibration curve once IOSACal v0.5 is released.\n\nRecords from the CSV file are used as input for a loop calibrating the input dates in the following radiocarbon calibration. The output is a short report and a probability distribution plot for each of the calibrations.\n\nTo prepare for this process we create variables to hold the inputs and outputs of the calibration in the following codeblock. first three lines of code define the variables *s*, *d*, & *e* as values from specified columns in our dataframe (*ID*, *date*, & *error*). The following two lines initialize empty lists, *cal68* and *cal95*, that will store the minimum and maximum date of each confidence interval for each sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create variables for sample id, date, and date error\ns = AppendixA.ID.values\nd = AppendixA.date.values\ne = AppendixA.error.values\n\n#create a list to hold min and max values of probability distributions\ncal68 = []\ncal95 = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot worthy of explaination happens in the codeblock.\n\nBeginning with line 1, the following text described the process of our loop line by line.\n\nLine 1 is the header of our **for** loop. The header defines the parameters of our loop, and describes the conditions to be met **for** the loop to stop. Our conditions state iterate the body of the loop below **for** a number of times equal to the <b>range</b> of <b>axis</b> 0, or rows, in <i>AppendixA</i>. We have three samples, so the loop will iterate a total of three times with the provided data.<br>\n\nLine 2 begins the body of the loop by defining a variable called <i>r</i> as equal to the values of date (<i>d</i>), error (<i>e</i>), and ID (<i>s</i>). The variables are designated as radiocarbon dates with the variable <i>R</i> (shorthand in IOSACal). The <i>i</i> in square brackets following <i>d</i>, <i>e</i>, and <i>s</i> is a reference to the value of <i>date</i>, <i>error</i>, and <i>ID</i> for the record with an index matching the current iteration of the loop. The <i>r</i> variable now holds a list containing the three values for each of the three samples. <br>\n\nLine 3 passes the values of our list stored in <i>r</i> to the <b>calibrate</b> method in IOSACal. By prefixing <b>r.</b> to <b>calibrate</b> we are defining that the values in *r* will will be calibrated using the <i>marine20</i> .14c file in parentheses. The calculated values are saved in the variable <i>cal_r</i>. <br>\n\nLine 4 creates a report for the calibration, saved to the variable - <i>report</i>.<br>\n\nOn line 5, we search the output report text for digits, storing them in as a list in the variable called <b>findDigits</b>. Line 6 and 7 append the min and max values from our <b>findDigits</b> list for the 68% and 95% confidence intervals, based on their position in the list. These lines take advantage of the consistent structure IOSACal uses for reporting to allow us to directly use the outputs in further analysis.<br>\n\nLines 8 and 9 then **print** the report stored in our *report* variable below our code, and creates and saves a visualization of the calibration to the directory - */SupplementaryData/output* as a JPG image named by *ID*."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dir= r\"./output/\"\nfor i in range(AppendixA.shape[0]):\n    r = R(d[i], e[i], s[i])\n    cal_r = r.calibrate('marine13')\n    report = (single_text(cal_r))\n    findDigits = [int(s) for s in re.findall(r'\\b\\d+\\b', report)]\n    cal68.append(findDigits[7:9])\n    cal95.append(findDigits[13:15])\n    print(report)\n    iplot(cal_r, output = dir + s[i] + \".jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After performing radiocarbon calibration, the output lists are cast to a `numpy array` for storage as float values. These values are put into a `for` loop that iterates for `i` number of times. The number of iterations is set using the `range()` function with an input parameter of a list of values. For flexibility on the number of records that can be input to this code this value is equal the the number of records in our input table by calling the shape of any column with `AppendixA.shape[0]` (here we chose the first column `0`).\n\nWithin the `for` loop, simple calculations run to retrieve the mean values of each output range and associated uncertainty. These values are appended to the variables `cal95Date`,  `cal95Mean`,  `cal95Err`, `d`, and `e` for use later in the workflow."},{"metadata":{"trusted":true},"cell_type":"code","source":"cal68Date = np.array(cal68)\ncal95Date = np.array(cal95)\ncal95Mean = np.array(d)\ncal95Err = np.array(e)\n\nfor i in range(AppendixA.shape[0]):\n    d[i] = (cal68Date[i,0]+cal68Date[i,1])/2\n    e[i] = (cal68Date[i,0]-cal68Date[i,1])/2\n    cal95Mean[i] = (cal95Date[i,0]+cal95Date[i,1])/2\n    cal95Err[i] = (cal95Date[i,0]-cal95Date[i,1])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting values are used to modify the formatted *Table* dataframe shown after running the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Table = AppendixA.style.hide_index().format({\"error\": lambda x: \"±{:.1f}\".format(abs(x))}, na_rep=\"-\").format({\"date\": lambda x: \"{:.0f} CalBP\".format(float(x))}).format({\"core_depth\": lambda x: \"{:.2f}m\".format(abs(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_corrected\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"modern_HHW\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_error\": lambda x: \"±{:.2f}m\".format(float(x))}, na_rep=\"-\").set_properties(subset=[\"error\", \"core_depth\", \"modern_HHW\", \"HHW_corrected\", \"HHW_error\"], **{'text-align': 'center'}).set_properties(subset=[\"date\"], **{'text-align': 'left'})\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results of the radiocarbon calibration are viewed in more detail below. The following codeblock creates and displays a new dataframe formatted to show date range, mean, and uncertainty at both confidence intervals for all samples calibrated."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['2'+'\\u03C3'+' Max Age', '2'+'\\u03C3'+' Min Age']\ncols2 = ['1'+'\\u03C3'+' Max Age', '1'+'\\u03C3'+' Min Age']\ncols3 = ['2'+'\\u03C3'+' Mean Age', '2'+'\\u03C3'+' Uncertainty', '1'+'\\u03C3'+' Mean Age', '1'+'\\u03C3'+' Uncertainty'] \n\ncalMean = np.column_stack((cal95Mean,cal95Err,d,e))\n\ncalDates = pd.DataFrame(cal95)\ncalDates.columns = cols\ncalDates.reindex(columns=calDates.columns.tolist() + cols2)\ncalDates[cols2] = cal68\ncalDates.reindex(columns=calDates.columns.tolist() + cols3)\ncalDates[cols3] = calMean.reshape(3,4)\ncalDates.insert(0, \"Sample ID\", AppendixA.iloc[0:3,0])\n\ncalDates.style.hide_index().set_properties([], **{'text-align': 'center'}).format({'2'+'\\u03C3'+' Max Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'2'+'\\u03C3'+' Min Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'1'+'\\u03C3'+' Mean Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'2'+'\\u03C3'+' Mean Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'1'+'\\u03C3'+' Max Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'1'+'\\u03C3'+' Min Age': lambda x: \"{:.0f} CalBP\".format(float(x))}).format({'2'+'\\u03C3'+' Uncertainty': lambda x: \"±{:.0f} \".format(float(x))}).format({'1'+'\\u03C3'+' Uncertainty': lambda x: \"±{:.0f} \".format(float(x))}).set_properties(subset=['2'+'\\u03C3'+' Uncertainty','1'+'\\u03C3'+' Uncertainty'], **{'text-align': 'left'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Elevation Correction of Sea Level Proxy Samples"},{"metadata":{},"cell_type":"markdown","source":"### Calculating Sample Elevation"},{"metadata":{},"cell_type":"markdown","source":"With the radiocarbon calibration complete, we can now begin to produce the geochronological *terminus ante quem* boundaries. Our first step is to calculate the elevation of the radiocarbon dated samples. These elevations will be used to derive past HHW in the CGVD2013 vertical datum.\n\nThe codeblock below subtracts *core_depth* from *modern_HHW* and places the result in the *HHW_corrected* column."},{"metadata":{"trusted":true},"cell_type":"code","source":"AppendixA[\"HHW_corrected\"] = AppendixA[\"modern_HHW\"]-AppendixA[\"core_depth\"]\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One sample, <i>Gx-6811</i>, was located within high marsh Zone 1b. The upper limit of Zone 1b will be 20cm below the limit of HHW mesasured from the top of Zone 1a.\n\nThe errors provided for <i>Gx-6811</i> cannot be reduced, but we can correct elevation to match the marine limit by adding the 20cm range in elevation for Zone 1a to the Zone 1b sample elevation.\n\nThe codeblock below performs this correction, then displays the updated values in our <i>AppendixA</i> dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"AppendixA.iloc[1:2,5] = AppendixA.iloc[1:2,5]+0.2\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Calculating Sea Level Rise (Higher High Water)"},{"metadata":{},"cell_type":"markdown","source":"Our calculated HHW values for the Kingsport sample dates are shown above.\n\nNext step is to determine the amount of RSL rise that has occurred since 1980 at the Kingsport sample locations. In order to do this, two coordinate pairs are required. Variables are defined below for our <i>x1</i>, <i>y1</i>, <i>x2</i>, and <i>y2</i> coordinates. Variable <i>x1</i> represents the radiocarbon equivalent date for the year 2020, where the value 0 is equal to the year 1950."},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = -70\nx2 = AppendixA.loc[2].at[\"date\"]\ny1 = AppendixA.loc[2].at[\"modern_HHW\"]\ny2 = AppendixA.loc[2].at[\"HHW_corrected\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use our coordinate pairs from our table to derive the equation for the linear relationship between the most recent of the radiocarbon dated samples (*Gx-6812, HHW_corrected*) and modern HHW for that sample (*Gx-6812, modern_HHW*) following a  slightly modified codeblock [\"from Stack Overflow\"](https://stackoverflow.com/a/21566184). This equation will be used to solve for the HHW in the year 1980 by solving for x = -30 in the two following codeblocks. \n\nThe first codeblock creates an object called *coords* from our *x1, y1*, and *x2, y2* coordinate pairs of date and elevation. We then use <b>zip()</b> to sort these pairs into *x_coords* and *y_coords* lists. The third line creates the coefficient matrix (*A*) for the least squares method, <b>lstsq()</b>, on line 4. The two derived values are the slope (<i>m</i>) and intercept (<i>c</i>) of our equation, printed below."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"coords = [(x1,y1),(x2,y2)]\nx_coords, y_coords = zip(*coords)\nA = vstack([x_coords,ones(len(x_coords))]).T\nm, c = lstsq(A, y_coords, rcond=None)[0]\nprint(\"Equation is y = {:.5f}x + {:.5f}\".format(m,c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our output equation is: $$y = -0.00230x + 6.63404$$<br>\n\nThe following two codeblocks perform two calculations. First, a modeled HHW elevation value is calaculated for Kingsport in 1980 by solving for <i>y</i>, where: $$x = -30$$<br>  Next, RSL rise since 1980 at HHW is calculated from the difference in HHW compared to 2020.<br>\n\nThe following codeblock can also be expressed as: $$HHW_{1980} = 0.00230(-30) + 6.63404$$<br>\n\nOur sea level rise calculation in the second codeblock is expressed as: $$RSLR_{[1980,2020]} = HHW_{2020} - (0.00230(-30) + 6.6340)$$<br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"SL = m * (-30) + c\n\nprint(\"Higher high water was\", \"{:.3f}m\".format(SL),\"CGVD2013 at Kingsport in 1980.\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"SLR = y1 - SL\nSLRcm = SLR*100\n\nprint(\"Sea level has risen\", \"{:.1f}cm\".format(SLRcm),\"since 1980 at Kingsport.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As our <i>HHW_corrected</i> elevations for the Kingsport core samples are calculated as values for the year 2020, we need to correct the elevations to match depth below 1980 HHW. The calculation is performed in the codeblock below and expressed as: $$HHW\\_corrected_{AppendixA} = HHW\\_corrected_{AppendixA} - SLR$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"AppendixA[\"HHW_corrected\"] = AppendixA[\"HHW_corrected\"] - SLR\nTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Calculating Equvalent Past HHW at Oak Point"},{"metadata":{},"cell_type":"markdown","source":"An equivalent HHW elevation at Oak Point is obtained for each calibrated date by correcting for SLR and sample elevation.\n\nAn offset value for the geochronological boundary elevations at Oak Point is provided by the difference in RSL rise for the Kingsport sample because we can expect our high-marsh indicators to increase in elevation at the rate of RSL rise. \n\nThe following codeblock calculates equivalent modern HHW elevations for Oak Point.\n\nOn line 1 the HHW value of <b>6.826</b> is used to calculate elevations for each sample subtracting <i>core_depth</i> and <i>SLR</i>. \n\nLine 2 displays the list of calculated values within <i>OakPointHHW</i>."},{"metadata":{"trusted":true},"cell_type":"code","source":"OakPointHHW = [6.826 - AppendixA.iloc[0:3,3] - SLR]\nOakPointHHW","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following codeblock creates a new dataframe called <i>OakPoint</i> holding the calculated HHW elevations, calibrated dates, and errors at Oak Point.\n\nLine 1 creates the dataframe from our <i>AppendixA</i> dataframe.\n\nLine 2 replaces the <i>AppendixA</i> data originally in the <i>OakPoint</i> dataframe with values from the <i>OakPointHHW</i> list calculated above. \n\nLine 3 drops the unnecessary fields from the <i>AppendixA</i> dataframe.\n\nLine 4 corrects the elevation for Zone 1b at Oak Point by adding the offset of 0.2m to the elevation of the second record.\n\nLines 5 and 6 stylize out table, and display the result."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"OakPoint = pd.DataFrame(data=AppendixA)\nOakPoint.iloc[0:3,5] = OakPointHHW[0]\nOakPoint = OakPoint.drop(['ID','core_depth','modern_HHW'], axis=1)\nOakPoint.iloc[1:2,2] = OakPoint.iloc[1:2,2] + 0.2\nOakPointTable = OakPoint.style.hide_index().format({\"error\": lambda x: \"±{:.1f}\".format(abs(x))}, na_rep=\"-\").format({\"date\": lambda x: \"{:.1f} CalBP\".format(float(x))}).format({\"HHW_corrected\": lambda x: \"{:.3f}m\".format(float(x))}, na_rep=\"-\").format({\"HHW_error\": lambda x: \"±{:.2f}m\".format(float(x))}, na_rep=\"-\").set_properties(subset=[\"error\", \"HHW_corrected\", \"HHW_error\"], **{'text-align': 'center'}).set_properties(subset=[\"date\"], **{'text-align': 'left'})\nOakPointTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Proxy Dating Rooted Stumps in Drowned Forest"},{"metadata":{},"cell_type":"markdown","source":"The *OakPoint* dataframe, shown above, holds HHW elevations at calibrated radiocarbon date intervals derived from samples located at Kingsport Marsh. \n\nIn the following codeblock three json files are added to the map as new layers. The files contain polyline contours at the mean elevation of the *geochronological boundaries* produced using the MARINE20 calibration curve for sample *Gx-6810*. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#load geojson file containing geochronological boundary data\nwith open(r\"./data/HHW4460BP.geojson\", 'r') as f:\n    rsl1 = json.load(f)\nwith open(r\"./data/HHW2550BP.geojson\", 'r') as f:\n    rsl2 = json.load(f)\nwith open(r\"./data/HHW1890BP.geojson\", 'r') as f:\n    rsl3 = json.load(f)\n\n#reference GeoJSON widgets as variables\nhhw4460 = GeoJSON(data=rsl1, name=\"4460 BP High Water Limit\", style={'opacity': 1, 'weight': 1, 'color': '#80bfff'})\nhhw2550 = GeoJSON(data=rsl2, name=\"2550 BP High Water Limit\", style={'opacity': 1, 'weight': 1, 'color': '#0099cc'})\nhhw1890 = GeoJSON(data=rsl3, name=\"1890 BP High Water Limit\", style={'opacity': 1, 'weight': 1, 'color': '#000000'})\n\n#add data to map frame\nmapFrame.add_layer(hhw4460)\nmapFrame.add_layer(hhw2550)\nmapFrame.add_layer(hhw1890)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surveyed points have been gathered at Oak Point and throughout the mudflats of Minas Basin. Below we will load and visualize some survey data in the preexisting map frame.\n\nIn order to visualize submerged landscape features in relation to the modelled high water limits, surveyed points of many features were converted of a GeoJSON file. Below we open the GeoJSON file and view it as a table."},{"metadata":{},"cell_type":"markdown","source":"The table above allows scrolling to the side and displays attributes of the first json file.\n\nNext the features can be plotted on our previous map frame. However, to differentiate the values a `Popup` method is used to visualize the `properties.Code` property of the points when a point is clicked. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#load geojson file containing surveyed but undated rsl indicator points\nwith open(r\"./data/rslPoints.geojson\", 'r') as f:\n    data = json.load(f)\n\n#reference geojson feature ID as variable 'features'\nfeatures = data['features']\nindElev = []\nzErr = []\n\n#create layer group referencing json records within the map frame\nrslGroup = LayerGroup(name=\"RSL Indicators\", layers=())\n\n#loop over geojson features\nfor i in range(len(features)):\n    location=(features[i]['geometry']['coordinates'][1],features[i]['geometry']['coordinates'][0])\n    indicators = features[i]['properties']['Code']\n    indElev.append(features[i]['properties']['OHeight'])\n    zErr.append(features[i]['properties']['zError'])\n    ind = str(indicators)\n    html = \"\"\"<p><b>Indicator type: </b>\"\"\" + f\"{ind}\" + \"\"\"</p>\"\"\"\n    rslPoint = CircleMarker( radius=4, color=\"#000000\", location=location, weight=1, opacity=1, \n                             fill_opacity=0.8, fill_color=\"#80bfff\", draggable=False)\n    \n    # Popup associated to a layer\n    rslPoint.popup = HTML(html)\n    rslGroup.add_layer(rslPoint)\n\nmapFrame.add_layer(rslGroup)\n#call and display the map frame\nmapFrame   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following were used to produce a plot of surveyed locations of stumps in the drowned forest in relation to sea level rise over-time. The cell below migrates calculated values from the *OakPoint* data frame in to a list. Current organization of the workflow required modern high water elevation and errors to be manually entered. Though this can be easily changed in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(OakPoint['date'])\nx.append(-70)\ndateErr = list(OakPoint['error'])\ndateErr.append(0)\ny = list(OakPoint['HHW_corrected'])\ny.append(6.826)\nerror = list(OakPoint['HHW_error'])\nerror.append(0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we create a plot using the same custom style used in the associated article. \n\nThe cell below converts the lists above in to `numpy array` objects, obtains the values from the geojson indicator points we stored in variables previously, and plots the values using custom style parameters."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#use the seaborn style colorblind in the plot\nplt.style.use('seaborn-colorblind')\n\n#define method to round values to the nearest 5\ndef myround(x, base=5):\n    return int(base * round(float(x)/base))\n\n#define x, y, errors, indicator elevation, z-error, calculated date, and date error\nx = np.array(x)\ndateErr = np.array(dateErr)\ny = np.array(y)\nerror = np.array(error)\nindAge = np.array([1976.758241758242, 2022.1794871794873, 2146.7216117216117, 2158.443223443224, 2214.4871794871797, 2469.0659340659345, 2503.864468864469, 2585.688622754491, 1001.7565217391303, 1415.2347826086955, 1537.408695652174])\n\n#plot points, errorbars, interpolated rsl indicator age, and errorbar bounds\nplt.plot(x, y, 'bo', x, y, 'k', linewidth=1.25, linestyle='dashed')\nplt.plot(indAge, indElev, 'd', color='#4CE600', markersize=6, markeredgewidth=0.75, markeredgecolor='k')\nplt.fill_betweenx(y, x-dateErr, x+dateErr, alpha=0.4, edgecolor='#A80000', facecolor='#A83800',\n    linewidth=1.5, linestyle='dashdot', antialiased=True)\nplt.errorbar(x,y,error,dateErr, alpha=0.6, color='#1B2ACC', linewidth=0, elinewidth=1.5, ecolor='black', capsize=5)\nplt.fill_between(x, y-error, y+error, alpha=0.75, edgecolor='#1B2ACC', facecolor='#51A8C7',\n    linewidth=1.25, linestyle='dashdot', antialiased=True)\n\n#stylize ticks, gridlines, bounds, and axis labels\nplt.xticks(np.arange(0,5000, step=500))\nplt.yticks(np.arange(-4,7.5, step=0.5))\nplt.axhline(-4,color='black')\nplt.axvline(-250,color='black')\nplt.axhline(7.5,color='black')\nplt.axvline(5000,color='black')\nplt.tick_params(direction='in', length=6, width=1, colors='black',grid_color='gray', grid_alpha=0.5)\nplt.xlabel('Age (years CalBP)')\nplt.ylabel('Elevation (metres CGVD2013)')\nplt.xlim(-250, 5000)\nplt.ylim(-4, 7.5)\n\n# Show the major grid lines with dark grey lines\nplt.grid(b=True, which='major', color='#666666')\n\n# Show the minor grid lines with very faint and almost transparent grey lines\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='#999999', alpha=0.2)\n\nfig = plt.gcf()\nfig.set_size_inches(20, 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result of our above plot shows the relative sea-level curve for Oak Point. The calibrated dates were produced with the calibration curve used in the above process, and elevations were corrected using HHW points from a CHS grid surface for higher high water. Uncertainties associated with each point are shown with vertical and horizontal bars. \n\nA linear model is used to show interpolated uncertainty bounds over time. The red boundary relates to uncertainty of the radiocarbon calibration in the 68% confidence interval, while the blue boundary relates to the uncertainty associated with the *Foram.* assemblage derived relative sea level indicator points."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}